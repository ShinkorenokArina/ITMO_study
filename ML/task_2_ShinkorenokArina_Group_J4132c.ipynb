{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2\n",
        "\n",
        "## Done by Arina Shinkorenok, group j4132c\n",
        "\n",
        "**Description:**\n",
        "\n",
        "1.\tDownload Alice in Wonderland by Lewis Carroll from Project Gutenberg's website http://www.gutenberg.org/files/11/11-0.txt\n",
        "2.\tPerform any necessary preprocessing on the text, including converting to lower case, removing stop words, numbers / non-alphabetic characters, lemmatization.\n",
        "3.\tFind Top 10 most important (for example, in terms of TF-IDF metric) words from each chapter in the text (not \"Alice\"); how would you name each chapter according to the identified tokens?\n",
        "4.\tFind the Top 10 most used verbs in sentences with Alice. What does Alice do most often?\n"
      ],
      "metadata": {
        "id": "ygpUvzJ4CiQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloaded the necessary libraries and the Alice in Wonderland file."
      ],
      "metadata": {
        "id": "KD5tSldaGPDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "edvujG_evRiA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d0c126a-6f3c-4822-c012-96b1ce735c38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import tqdm\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "rwDO3UwrvRfm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "5i_4SbHO5HFf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2840430f-c13b-4c7d-ec97-1e14067fd0c2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-08 14:38:09.032912: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-08 14:38:09.032994: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-08 14:38:09.033028: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-08 14:38:10.292419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the text of the book and divide it into chapters"
      ],
      "metadata": {
        "id": "5nxQgzh2HBOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/ML-ITMO/11-0.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "mXCEsqdb45_f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chapters = text.split('CHAPTER')[13:]\n",
        "for chapter in chapters:\n",
        "    print(chapter[:25])"
      ],
      "metadata": {
        "id": "xvFTcFbh47kn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61319dae-2ae4-481c-d829-450dcb7a3811"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I.\n",
            "Down the Rabbit-Hole\n",
            "\n",
            " II.\n",
            "The Pool of Tears\n",
            "\n",
            "\n",
            "\n",
            " III.\n",
            "A Caucus-Race and a\n",
            " IV.\n",
            "The Rabbit Sends in \n",
            " V.\n",
            "Advice from a Caterpi\n",
            " VI.\n",
            "Pig and Pepper\n",
            "\n",
            "\n",
            "For\n",
            " VII.\n",
            "A Mad Tea-Party\n",
            "\n",
            "\n",
            "T\n",
            " VIII.\n",
            "The Queen’s Croque\n",
            " IX.\n",
            "The Mock Turtle’s St\n",
            " X.\n",
            "The Lobster Quadrille\n",
            " XI.\n",
            "Who Stole the Tarts?\n",
            " XII.\n",
            "Alice’s Evidence\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "1-oEDLplKuM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the English (en) core language model with a small size (web_sm) for English\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# processed text\n",
        "new_text = nlp(text.lower())"
      ],
      "metadata": {
        "id": "dm7N8Sji5Mmf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# go through the first 10 tokens in the text\n",
        "for token in new_text[:10]:\n",
        "    print(token.text, token.is_alpha, token.is_stop, token.lemma_, token.pos_)"
      ],
      "metadata": {
        "id": "P2l6fyKD5Mj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f49d65c4-8967-42ba-e790-b72c2cee37ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿the False False ﻿the DET\n",
            "project True False project NOUN\n",
            "gutenberg True False gutenberg PROPN\n",
            "ebook True False ebook PROPN\n",
            "of True True of ADP\n",
            "alice True False alice PROPN\n",
            "’s False True ’s PART\n",
            "adventures True False adventure NOUN\n",
            "in True True in ADP\n",
            "wonderland True False wonderland NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# processing the text of chapters and converting them into tokens\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "tokenized_chapters = []\n",
        "for chapter in tqdm.tqdm(chapters):\n",
        "    new_text = nlp(chapter)\n",
        "    tokens = ' '.join([token.lemma_.lower() for token in new_text if (token.is_alpha and not token.is_stop and not token.text == \"Alice\")])\n",
        "    tokenized_chapters.append(tokens)"
      ],
      "metadata": {
        "id": "gvDE4eXw5Mh_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96ca665b-37da-4bf4-d002-01fb77c893cd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:08<00:00,  1.35it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# applying TF-IDF vectorisation to chapter tokens\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(tokenized_chapters[0:])\n",
        "X = X.toarray()\n",
        "X.shape"
      ],
      "metadata": {
        "id": "JeOfK_7h5MdQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8045f8e-58a0-409f-82ac-b18198427a6e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12, 2093)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top 10 most important words from each chapter"
      ],
      "metadata": {
        "id": "hh93dAvgK9Ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the top 10 important words (according to the TF-IDF metric) for each chapter\n",
        "feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "for chapter_idx in range(X.shape[0]):\n",
        "    top_words = np.argsort(X[chapter_idx])[-10:][::-1]\n",
        "    print(f\"Top 10 words, chapter {chapter_idx + 1}: {feature_names[top_words]}\")"
      ],
      "metadata": {
        "id": "YTjxnoKF5MbB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c394af-a0da-4049-d752-d77ca7b8b084"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 words, chapter 1: ['fall' 'eat' 'think' 'little' 'bat' 'rabbit' 'door' 'key' 'go' 'way']\n",
            "Top 10 words, chapter 2: ['mouse' 'pool' 'little' 'say' 'oh' 'swam' 'cat' 'think' 'dear' 'cry']\n",
            "Top 10 words, chapter 3: ['say' 'mouse' 'dodo' 'race' 'lory' 'prize' 'know' 'dry' 'thimble' 'bird']\n",
            "Top 10 words, chapter 4: ['bill' 'little' 'window' 'rabbit' 'puppy' 'grow' 'glove' 'fan' 'say'\n",
            " 'bottle']\n",
            "Top 10 words, chapter 5: ['caterpillar' 'say' 'serpent' 'pigeon' 'youth' 'egg' 'size' 'think'\n",
            " 'father' 'little']\n",
            "Top 10 words, chapter 6: ['say' 'footman' 'cat' 'baby' 'mad' 'duchess' 'grin' 'sneeze' 'wow' 'go']\n",
            "Top 10 words, chapter 7: ['hatter' 'dormouse' 'say' 'hare' 'march' 'tea' 'twinkle' 'time' 'know'\n",
            " 'go']\n",
            "Top 10 words, chapter 8: ['queen' 'say' 'hedgehog' 'king' 'gardener' 'look' 'go' 'rose' 'soldier'\n",
            " 'cat']\n",
            "Top 10 words, chapter 9: ['turtle' 'say' 'mock' 'gryphon' 'duchess' 'moral' 'queen' 'go' 'think'\n",
            " 'school']\n",
            "Top 10 words, chapter 10: ['turtle' 'mock' 'gryphon' 'say' 'lobster' 'dance' 'soup' 'beautiful'\n",
            " 'whiting' 'oop']\n",
            "Top 10 words, chapter 11: ['king' 'hatter' 'say' 'court' 'dormouse' 'witness' 'jury' 'queen'\n",
            " 'officer' 'juror']\n",
            "Top 10 words, chapter 12: ['project' 'gutenberg' 'tm' 'work' 'electronic' 'foundation' 'copy'\n",
            " 'copyright' 'term' 'agreement']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How would you name each chapter according to the identified tokens?"
      ],
      "metadata": {
        "id": "t70QbMB2LF86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tried, but my imagination is not very good, don't judge strictly.....\n",
        "\n",
        "1. the little rabbit has the key to the door\n",
        "2. cat was swimming in the pool and crying\n",
        "3. mouse took the prize\n",
        "4. the little rabbit grew up\n",
        "5. a caterpillar is a small serpent\n",
        "6. Wow, it's a mad duchess!\n",
        "7. hatter and tea\n",
        "8. queen, king and soldiers\n",
        "9. turtle morality\n",
        "10. beautiful lobster dance\n",
        "11. court of the king and queen\n",
        "12. Gutenberg's work"
      ],
      "metadata": {
        "id": "aBTaQpkDLOWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top 10 most used verbs in sentences with Alice"
      ],
      "metadata": {
        "id": "iIYA2sokLOtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = nlp(text)"
      ],
      "metadata": {
        "id": "Em0MDR8U5MYk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the top 10 most common verbs in sentences mentioning \"Alice\"\n",
        "verbs = []\n",
        "\n",
        "for sent in new_text.sents:\n",
        "    get_verbs = False\n",
        "    for token in sent:\n",
        "        if token.text.lower() == \"alice\":\n",
        "            get_verbs = True\n",
        "            break\n",
        "\n",
        "    if get_verbs:\n",
        "        for token in sent:\n",
        "            if (token.pos_ == \"VERB\") and token.is_alpha and (not token.is_stop):\n",
        "                verbs.append(token.lemma_.lower())\n",
        "\n",
        "counter = Counter(verbs)\n",
        "print(f\"Top 10 verbs used with Alice: {counter.most_common(10)}\")"
      ],
      "metadata": {
        "id": "Wx75uwG66aMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a891f9d-5db4-41b3-f233-4abc9b993263"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 verbs used with Alice: [('say', 202), ('think', 87), ('go', 56), ('look', 51), ('know', 43), ('begin', 40), ('come', 33), ('get', 27), ('feel', 25), ('find', 23)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What does Alice do most often?"
      ],
      "metadata": {
        "id": "6VhM8gY-Lciw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most often Alice says, then thinks"
      ],
      "metadata": {
        "id": "F127zi2CLeIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this lab work, the text \"Alice in Wonderland\" was processed, the top 10 words in each chapter and the top 10 verbs that are used in sentences with Alice were found. I used the spaCy library which provides powerful tools for natural language processing and I had a new experience."
      ],
      "metadata": {
        "id": "TKCsl2d7VRuz"
      }
    }
  ]
}