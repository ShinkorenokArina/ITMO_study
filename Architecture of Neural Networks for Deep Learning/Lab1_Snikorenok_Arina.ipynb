{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13345,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":11047}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7817893e67d048859fc9d6c37564aa1e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_29b97d268eb84d428f9d2dbda49bb370","IPY_MODEL_3c6a73d0444b482a8b72bd2c96e3700a","IPY_MODEL_88e1c25a773c4e5db17954dcdc648220"],"layout":"IPY_MODEL_e18502b451844b478df3e2607a2d379d"}},"29b97d268eb84d428f9d2dbda49bb370":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7824cc0a24d04b85983a4673ca783298","placeholder":"​","style":"IPY_MODEL_84f919649659454e9a7c449b3641a741","value":"Loading checkpoint shards: 100%"}},"3c6a73d0444b482a8b72bd2c96e3700a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_911bd7fcbb9a44dd963ada4356d60db7","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2b20d1b7e7a34f179970532e22155cb5","value":3}},"88e1c25a773c4e5db17954dcdc648220":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81cc947f5b3945ed993ab8cf2a3bbf91","placeholder":"​","style":"IPY_MODEL_ce49273d775e45e48fed6ef86ee04af6","value":" 3/3 [00:26&lt;00:00,  8.13s/it]"}},"e18502b451844b478df3e2607a2d379d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7824cc0a24d04b85983a4673ca783298":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84f919649659454e9a7c449b3641a741":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"911bd7fcbb9a44dd963ada4356d60db7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b20d1b7e7a34f179970532e22155cb5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"81cc947f5b3945ed993ab8cf2a3bbf91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce49273d775e45e48fed6ef86ee04af6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Lab1\n**Done by Arina Shinkorenok J4132c**","metadata":{}},{"cell_type":"markdown","source":"## Download libraries","metadata":{}},{"cell_type":"code","source":"pip install --quiet transformers==4.37.2 accelerate==0.24.0 sentencepiece==0.1.99 optimum==1.13.2 peft==0.5.0 bitsandbytes==0.41.2.post2 datasets==2.14.7","metadata":{"id":"wL-MGqcNIJZx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, TrainingArguments\nfrom tqdm.auto import tqdm, trange\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport peft\n\nimport transformers\nfrom datasets import load_dataset\n\nimport random\nconst_seed = 100","metadata":{"id":"JwnrmYZRIJZ2","execution":{"iopub.status.busy":"2024-03-02T17:58:35.574718Z","iopub.execute_input":"2024-03-02T17:58:35.575390Z","iopub.status.idle":"2024-03-02T17:58:45.600366Z","shell.execute_reply.started":"2024-03-02T17:58:35.575357Z","shell.execute_reply":"2024-03-02T17:58:45.599525Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-02 17:58:40.818204: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-02 17:58:40.818265: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-02 17:58:40.819788: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"assert torch.cuda.is_available(), \"check out cuda availability (change runtime type in colab)\"","metadata":{"id":"33y-hAWcIJZ5","execution":{"iopub.status.busy":"2024-03-02T17:58:45.608142Z","iopub.execute_input":"2024-03-02T17:58:45.608508Z","iopub.status.idle":"2024-03-02T17:58:45.615467Z","shell.execute_reply.started":"2024-03-02T17:58:45.608473Z","shell.execute_reply":"2024-03-02T17:58:45.614635Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"mDjaWX8LIJZ7","execution":{"iopub.status.busy":"2024-03-02T17:58:45.617142Z","iopub.execute_input":"2024-03-02T17:58:45.617538Z","iopub.status.idle":"2024-03-02T17:58:45.625054Z","shell.execute_reply.started":"2024-03-02T17:58:45.617515Z","shell.execute_reply":"2024-03-02T17:58:45.624129Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"! ls","metadata":{"id":"mRnQ2wnQIJZ8"}},{"cell_type":"markdown","source":"# Part 0: Initializing the model and tokenizer","metadata":{"id":"_Fe5Ee10IJZ_"}},{"cell_type":"markdown","source":"let's take mistral model for our experiments (https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) that was tuned to follow user instructions. Pay attention that we load model in 4 bit to decrease the memory usage.","metadata":{"id":"mW9Rf1qUIJaB"}},{"cell_type":"code","source":"model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"","metadata":{"id":"JRkiUrUoIJaB","execution":{"iopub.status.busy":"2024-03-02T17:58:48.878132Z","iopub.execute_input":"2024-03-02T17:58:48.879041Z","iopub.status.idle":"2024-03-02T17:58:48.883210Z","shell.execute_reply.started":"2024-03-02T17:58:48.879005Z","shell.execute_reply":"2024-03-02T17:58:48.882093Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# load llama tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, device_map=device)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\n# Note: to speed up inference you can use flash attention 2 (https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    low_cpu_mem_usage=True,\n    offload_state_dict=True,\n    load_in_4bit=True,\n    torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n)\nfor param in model.parameters():\n    param.requires_grad=False\n\nmodel.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\nmodel.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174","metadata":{"id":"QAYbaUTwIJaD","outputId":"bef96511-4421-4502-cb43-5d726f4ab8ff","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["7817893e67d048859fc9d6c37564aa1e","29b97d268eb84d428f9d2dbda49bb370","3c6a73d0444b482a8b72bd2c96e3700a","88e1c25a773c4e5db17954dcdc648220","e18502b451844b478df3e2607a2d379d","7824cc0a24d04b85983a4673ca783298","84f919649659454e9a7c449b3641a741","911bd7fcbb9a44dd963ada4356d60db7","2b20d1b7e7a34f179970532e22155cb5","81cc947f5b3945ed993ab8cf2a3bbf91","ce49273d775e45e48fed6ef86ee04af6"]},"execution":{"iopub.status.busy":"2024-03-02T17:58:53.182105Z","iopub.execute_input":"2024-03-02T17:58:53.182856Z","iopub.status.idle":"2024-03-02T17:59:22.286866Z","shell.execute_reply.started":"2024-03-02T17:58:53.182814Z","shell.execute_reply":"2024-03-02T17:59:22.286015Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18ed746ef55f442685fcdbca5c8e6395"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Part 1 (5 points): Prompt-engineering","metadata":{"id":"ceuPv-6ZIJaG"}},{"cell_type":"markdown","source":"**There are different strategies for text generation in huggingface:**\n\n| Strategy | Description | Pros & Cons |\n| --- | --- | --- |\n| Greedy Search | Chooses the word with the highest probability as the next word in the sequence. | **Pros:** Simple and fast. <br> **Cons:** Can lead to repetitive and incoherent text. |\n| Sampling with Temperature | Introduces randomness in the word selection. A higher temperature leads to more randomness. | **Pros:** Allows exploration and diverse output. <br> **Cons:** Higher temperatures can lead to nonsensical outputs. |\n| Nucleus Sampling (Top-p Sampling) | Selects the next word from a truncated vocabulary, the \"nucleus\" of words that have a cumulative probability exceeding a pre-specified threshold (p). | **Pros:** Balances diversity and quality. <br> **Cons:** Setting an optimal 'p' can be tricky. |\n| Beam Search | Explores multiple hypotheses (sequences of words) at each step, and keeps the 'k' most likely, where 'k' is the beam width. | **Pros:** Produces more reliable results than greedy search. <br> **Cons:** Can lack diversity and lead to generic responses. |\n| Top-k Sampling | Randomly selects the next word from the top 'k' words with the highest probabilities. | **Pros:** Introduces randomness, increasing output diversity. <br> **Cons:** Random selection can sometimes lead to less coherent outputs. |\n| Length Normalization | Prevents the model from favoring shorter sequences by dividing the log probabilities by the sequence length raised to some power. | **Pros:** Makes longer and potentially more informative sequences more likely. <br> **Cons:** Tuning the normalization factor can be difficult. |\n| Stochastic Beam Search | Introduces randomness into the selection process of the 'k' hypotheses in beam search. | **Pros:** Increases diversity in the generated text. <br> **Cons:** The trade-off between diversity and quality can be tricky to manage. |\n| Decoding with Minimum Bayes Risk (MBR) | Chooses the hypothesis (out of many) that minimizes expected loss under a loss function. | **Pros:** Optimizes the output according to a specific loss function. <br> **Cons:** Computationally more complex and requires a good loss function. |\n\nDocumentation references:\n- [reference for `AutoModelForCausalLM.generate()`](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n- [reference for `AutoTokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)\n- Huggingface [docs on generation strategies](https://huggingface.co/docs/transformers/generation_strategies)","metadata":{"id":"q5gV5N9oIJaH"}},{"cell_type":"code","source":"# TODO: create a function for generation with huggingface\ndef get_answer(\n    tokenizer,\n    model,\n    messages,\n    max_new_tokens=200,\n    temperature=0.5,\n    do_sample=True,\n):\n    # TODO: tokenize input, generate answer and decode output. Pay attention to tokenizer methods\n    tokenized_input = tokenizer.apply_chat_template(\n        messages,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n    ).to(device)\n\n    model_output = model.generate(\n        tokenized_input,\n        max_new_tokens=max_new_tokens,\n        do_sample=do_sample,\n        top_k=10,\n        top_p=0.91,\n    )\n\n    return tokenizer.batch_decode(model_output, skip_special_tokens=True)","metadata":{"id":"zKFlHoO0IJaI","execution":{"iopub.status.busy":"2024-03-02T17:59:22.288578Z","iopub.execute_input":"2024-03-02T17:59:22.288910Z","iopub.status.idle":"2024-03-02T17:59:22.295015Z","shell.execute_reply.started":"2024-03-02T17:59:22.288873Z","shell.execute_reply":"2024-03-02T17:59:22.294075Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Let's try our model\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Write an explanation of tensors for 5 year old\",\n    },\n]\n\nprint(get_answer(tokenizer, model, messages)[0])","metadata":{"id":"hnOu2t0TIJaJ","outputId":"b6ce9557-a1fe-40c0-b17e-f04009508b3c","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-03-02T17:59:22.296429Z","iopub.execute_input":"2024-03-02T17:59:22.296875Z","iopub.status.idle":"2024-03-02T17:59:35.949405Z","shell.execute_reply.started":"2024-03-02T17:59:22.296842Z","shell.execute_reply":"2024-03-02T17:59:35.948408Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[INST] Write an explanation of tensors for 5 year old [/INST] Tensors are like magical boxes that can hold different things, but they have a special rule. They can hold numbers, but not just any numbers, they have to be organized in a certain way.\n\nImagine you have a box of apples, and you want to count how many apples you have. That's like having a number, which is easy to understand. But, what if your box can also hold not only apples but also oranges or bananas? And you want to know how many of each fruit you have. That's when you need a tensor.\n\nA tensor is like a special box with different sections. Each section can hold a different kind of fruit, and you can count how many of each kind of fruit you have in each section. So, a tensor with two sections might look like this: [2 apples, 3 oranges]. This is called a 2-dimensional tensor because it has 2 sections.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You should obtain an explanation from the model. If so, let us go further!","metadata":{"id":"MU0bAN4MIJaL"}},{"cell_type":"markdown","source":"Now we will take a sample from boolQ (https://huggingface.co/datasets/google/boolq) dataset and try prompting techniques to extract the needed answer and calculate its quality.","metadata":{"id":"BJpZB2P1IJaL"}},{"cell_type":"code","source":"df = load_dataset(\"google/boolq\")","metadata":{"scrolled":true,"id":"Lidah3gXIJaL","execution":{"iopub.status.busy":"2024-03-02T17:59:35.952259Z","iopub.execute_input":"2024-03-02T17:59:35.952908Z","iopub.status.idle":"2024-03-02T17:59:39.608028Z","shell.execute_reply.started":"2024-03-02T17:59:35.952880Z","shell.execute_reply":"2024-03-02T17:59:39.607232Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Fixing 20 validation examples\n\nrandom.seed(const_seed)\nidx = random.sample(range(1, 3270), 20)","metadata":{"id":"ipe_0Ei9IJaM","execution":{"iopub.status.busy":"2024-03-02T17:59:39.609186Z","iopub.execute_input":"2024-03-02T17:59:39.609539Z","iopub.status.idle":"2024-03-02T17:59:39.614173Z","shell.execute_reply.started":"2024-03-02T17:59:39.609508Z","shell.execute_reply":"2024-03-02T17:59:39.613230Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# sample you will work with\ndf_sample = df[\"validation\"].select(idx)","metadata":{"id":"D_EcSIvPIJaN","execution":{"iopub.status.busy":"2024-03-02T17:59:39.615390Z","iopub.execute_input":"2024-03-02T17:59:39.615723Z","iopub.status.idle":"2024-03-02T17:59:39.629077Z","shell.execute_reply.started":"2024-03-02T17:59:39.615692Z","shell.execute_reply":"2024-03-02T17:59:39.628219Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# For instance, you can construct your prompt the following way\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"\"\"You are given a text and question. Answer only \"true\" or \"false\".\ntext: As with other games in The Elder Scrolls series, the game is set on the continent of Tamriel. The events of the game occur a millennium before those of The Elder Scrolls V: Skyrim and around 800 years before The Elder Scrolls III: Morrowind and The Elder Scrolls IV: Oblivion. It has a broadly similar structure to Skyrim, with two separate conflicts progressing at the same time, one with the fate of the world in the balance, and one where the prize is supreme power on Tamriel. In The Elder Scrolls Online, the first struggle is against the Daedric Prince Molag Bal, who is attempting to meld the plane of Mundus with his realm of Coldharbour, and the second is to capture the vacant imperial throne, contested by three alliances of the mortal races. The player character has been sacrificed to Molag Bal, and Molag Bal has stolen their soul, the recovery of which is the primary game objective.\nquestion: is elder scrolls online the same as skyrim\nanswer: \"\"\"\n    },\n]\n\nprint(get_answer(tokenizer, model, messages)[0])","metadata":{"id":"IxUHi9QEIJaO","outputId":"6f62ee82-5052-4d01-f43d-23fc5023cc4b","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-03-02T17:59:39.630296Z","iopub.execute_input":"2024-03-02T17:59:39.630635Z","iopub.status.idle":"2024-03-02T17:59:43.328124Z","shell.execute_reply.started":"2024-03-02T17:59:39.630603Z","shell.execute_reply":"2024-03-02T17:59:43.327109Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[INST] You are given a text and question. Answer only \"true\" or \"false\".\ntext: As with other games in The Elder Scrolls series, the game is set on the continent of Tamriel. The events of the game occur a millennium before those of The Elder Scrolls V: Skyrim and around 800 years before The Elder Scrolls III: Morrowind and The Elder Scrolls IV: Oblivion. It has a broadly similar structure to Skyrim, with two separate conflicts progressing at the same time, one with the fate of the world in the balance, and one where the prize is supreme power on Tamriel. In The Elder Scrolls Online, the first struggle is against the Daedric Prince Molag Bal, who is attempting to meld the plane of Mundus with his realm of Coldharbour, and the second is to capture the vacant imperial throne, contested by three alliances of the mortal races. The player character has been sacrificed to Molag Bal, and Molag Bal has stolen their soul, the recovery of which is the primary game objective.\nquestion: is elder scrolls online the same as skyrim\nanswer:  [/INST] False. While there are similarities in the structure of the game and the setting of Tamriel, The Elder Scrolls Online and The Elder Scrolls V: Skyrim are different games with distinct stories and objectives.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Is anything wrong with the output? Now it is time for you to play around and try to come up with some better prompt.","metadata":{"id":"RxIRKzyyIJaO"}},{"cell_type":"code","source":"def generate_prompt(gen_type, row, examples=None):\n    question = row[\"question\"]\n    passage = row[\"passage\"]\n    true_answer = row[\"answer\"]\n    if gen_type == \"naive\":\n        instruction = 'You are given a text and a question to answer. Write only the answer on a separate empty line in the format: \"answer=true\" or \"answer=false\".\\n'\n    elif gen_type == \"few_shot\":\n        instruction = 'You are given a text and a question to answer. Write only the answer on a separate empty line in the format: \"answer=true\" or \"answer=false\". Examples:\\n'\n        instruction += examples\n    elif gen_type == \"cot\":\n        instruction = 'You are given a text and a question to answer. Write only the answer on a separate empty line in the format: \"answer=true\" or \"answer=false\". Try to think step by step.\\n'\n\n    cot_string = \"Let's think step by step\\n\" if gen_type == \"cot\" else \"\"\n    prompt = {\n        \"role\": \"user\",\n        \"content\": f\"instruction: {instruction}\\ntext: {passage}\\nquestion: {question}?\\n{cot_string}\"\n    }\n    return prompt, true_answer","metadata":{"id":"eT6UDH34IJaP","execution":{"iopub.status.busy":"2024-03-02T17:59:43.329358Z","iopub.execute_input":"2024-03-02T17:59:43.329727Z","iopub.status.idle":"2024-03-02T17:59:43.336410Z","shell.execute_reply.started":"2024-03-02T17:59:43.329699Z","shell.execute_reply":"2024-03-02T17:59:43.335433Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def extract_answer(model_output):\n    inst_id = model_output.find(\"[/INST]\")\n    trunced = model_output[inst_id+len(\"[/INST]\"):]\n    trunced_list = trunced.split(\" \")\n\n    answer = False\n\n    for i in trunced_list:\n        if i.startswith(\"answer=\"):\n            if \"true\" in i:\n                answer = True\n                break\n    return answer","metadata":{"id":"o36_KmCWIJaP","execution":{"iopub.status.busy":"2024-03-02T17:59:43.337610Z","iopub.execute_input":"2024-03-02T17:59:43.337897Z","iopub.status.idle":"2024-03-02T17:59:43.350431Z","shell.execute_reply.started":"2024-03-02T17:59:43.337857Z","shell.execute_reply":"2024-03-02T17:59:43.349541Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def make_naive_predict(test_ds, model, get_answer_fn=get_answer):\n    predicted_labels = []\n    true_labels = []\n    gen_res = []\n\n    for row in test_ds:\n        prompt, true_answer = generate_prompt(gen_type=\"naive\", row=row)\n\n        model_output = get_answer_fn(tokenizer, model, [prompt])[0]\n\n        answer = extract_answer(model_output)\n\n        predicted_labels.append(answer)\n        true_labels.append(true_answer)\n        gen_res.append({\"prompt\": prompt, \"output\": model_output, \"true_ans\": true_answer, \"pred_ans\": answer})\n\n\n    return torch.tensor(predicted_labels), torch.tensor(true_labels), gen_res","metadata":{"id":"8HDmXzQ7IJaP","execution":{"iopub.status.busy":"2024-03-02T17:59:43.354134Z","iopub.execute_input":"2024-03-02T17:59:43.355002Z","iopub.status.idle":"2024-03-02T17:59:43.361870Z","shell.execute_reply.started":"2024-03-02T17:59:43.354969Z","shell.execute_reply":"2024-03-02T17:59:43.360656Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def example_format(text, question, answer):\n        return f\"text: {text}\\nquestion: {question}?\\nanswer={str(answer).lower()}\"\n\ndef gen_examples(dataset, n_shots):\n    ids = random.sample(range(1, 9427), n_shots)\n    sample = dataset.select(ids)\n    examples = \"\"\n\n    for s in sample:\n        examples += example_format(s[\"passage\"], s[\"question\"], s[\"answer\"])\n\n    return examples","metadata":{"id":"TsVHw3qnIJaQ","execution":{"iopub.status.busy":"2024-03-02T17:59:43.362952Z","iopub.execute_input":"2024-03-02T17:59:43.363313Z","iopub.status.idle":"2024-03-02T17:59:43.371870Z","shell.execute_reply.started":"2024-03-02T17:59:43.363282Z","shell.execute_reply":"2024-03-02T17:59:43.370920Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def few_shot_prompting(test_ds, model, add_ds=df[\"train\"], n_shots=5, get_answer_fn=get_answer):\n    predicted_labels = []\n    true_labels = []\n    gen_res = []\n\n    for row in test_ds:\n        examples = gen_examples(add_ds, n_shots)\n        prompt, true_answer = generate_prompt(gen_type=\"few_shot\", row=row, examples=examples)\n\n        model_output = get_answer_fn(tokenizer, model, [prompt])[0]\n\n        answer = extract_answer(model_output)\n\n        predicted_labels.append(answer)\n        true_labels.append(true_answer)\n        gen_res.append({\"prompt\": prompt, \"output\": model_output, \"true_ans\": true_answer, \"pred_ans\": answer})\n\n\n    return torch.tensor(predicted_labels), torch.tensor(true_labels), gen_res","metadata":{"id":"Hu0VG8MNIJaQ","execution":{"iopub.status.busy":"2024-03-02T17:59:43.373126Z","iopub.execute_input":"2024-03-02T17:59:43.374525Z","iopub.status.idle":"2024-03-02T17:59:43.383986Z","shell.execute_reply.started":"2024-03-02T17:59:43.374487Z","shell.execute_reply":"2024-03-02T17:59:43.383033Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def cot_prompting(test_ds, model, get_answer_fn=get_answer):\n    predicted_labels = []\n    true_labels = []\n    gen_res = []\n\n    for row in test_ds:\n        prompt, true_answer = generate_prompt(gen_type=\"cot\", row=row)\n\n        model_output = get_answer_fn(tokenizer, model, [prompt])[0]\n\n        answer = extract_answer(model_output)\n\n        predicted_labels.append(answer)\n        true_labels.append(true_answer)\n        gen_res.append({\"prompt\": prompt, \"output\": model_output, \"true_ans\": true_answer, \"pred_ans\": answer})\n\n    return torch.tensor(predicted_labels), torch.tensor(true_labels), gen_res","metadata":{"id":"0XuMTzKFIJaR","execution":{"iopub.status.busy":"2024-03-02T17:59:43.385210Z","iopub.execute_input":"2024-03-02T17:59:43.386209Z","iopub.status.idle":"2024-03-02T17:59:43.392995Z","shell.execute_reply.started":"2024-03-02T17:59:43.386171Z","shell.execute_reply":"2024-03-02T17:59:43.392215Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"orig_naive_pred, orig_naive_true, orig_naive_gen = make_naive_predict(df_sample, model)","metadata":{"id":"toK462zbIJaR","outputId":"8f11eecf-a33e-49ea-ac23-b6fce6f07a02","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-03-02T17:59:43.394885Z","iopub.execute_input":"2024-03-02T17:59:43.395169Z","iopub.status.idle":"2024-03-02T18:00:37.776395Z","shell.execute_reply.started":"2024-03-02T17:59:43.395128Z","shell.execute_reply":"2024-03-02T18:00:37.775562Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"orig_fw_pred, orig_fw_true, orig_fw_gen = few_shot_prompting(df_sample, model)","metadata":{"id":"_Uj8dQvaIJaR","outputId":"8ed8f9fe-752b-40dc-f4c1-44c8374b8b42","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-03-02T18:00:37.777587Z","iopub.execute_input":"2024-03-02T18:00:37.777879Z","iopub.status.idle":"2024-03-02T18:02:55.467256Z","shell.execute_reply.started":"2024-03-02T18:00:37.777854Z","shell.execute_reply":"2024-03-02T18:02:55.466228Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"orig_cot_pred, orig_cot_true, orig_cot_gen = cot_prompting(df_sample, model)","metadata":{"id":"r-06CwU-IJaT","outputId":"4011d666-36f8-4dd4-9422-10c8f8baf229","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-03-02T18:02:55.468414Z","iopub.execute_input":"2024-03-02T18:02:55.468689Z","iopub.status.idle":"2024-03-02T18:03:58.224495Z","shell.execute_reply.started":"2024-03-02T18:02:55.468666Z","shell.execute_reply":"2024-03-02T18:03:58.223635Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nimport pandas as pd","metadata":{"id":"f9qsBTq8IJaT","execution":{"iopub.status.busy":"2024-03-02T18:03:58.225710Z","iopub.execute_input":"2024-03-02T18:03:58.226007Z","iopub.status.idle":"2024-03-02T18:03:58.230726Z","shell.execute_reply.started":"2024-03-02T18:03:58.225982Z","shell.execute_reply":"2024-03-02T18:03:58.229758Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# TODO: create function to evaluate answers\n# Note: you can adapt function for different answer structures,\n# but you should be able to automatically extract the target \"true\" or \"false\" components\ndef evaluate_answers(true_ans, pred_ans):\n    titles = [\"naive\", \"few_shot\", \"chain_of_thought\"]\n    result = []\n    for true, pred, title in zip(true_ans, pred_ans, titles):\n        acc = accuracy_score(true, pred)\n        result.append({\"title\": title, \"accuracy\": acc})\n    metrics = pd.DataFrame(result, columns=[\"title\", \"accuracy\"]).set_index(\"title\")\n    print(metrics)\n    return metrics","metadata":{"id":"NRnaDVoIIJaT","execution":{"iopub.status.busy":"2024-03-02T18:03:58.232083Z","iopub.execute_input":"2024-03-02T18:03:58.232614Z","iopub.status.idle":"2024-03-02T18:03:58.244463Z","shell.execute_reply.started":"2024-03-02T18:03:58.232587Z","shell.execute_reply":"2024-03-02T18:03:58.243563Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"orig_metrics = evaluate_answers([orig_naive_true, orig_fw_true, orig_cot_true], [orig_naive_pred, orig_fw_pred, orig_cot_pred])","metadata":{"id":"edMmu53SN0x5","outputId":"178e2118-5990-42d7-af25-c66f1d444f55","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-03-02T18:03:58.245551Z","iopub.execute_input":"2024-03-02T18:03:58.246385Z","iopub.status.idle":"2024-03-02T18:03:58.282362Z","shell.execute_reply.started":"2024-03-02T18:03:58.246354Z","shell.execute_reply":"2024-03-02T18:03:58.281381Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"                  accuracy\ntitle                     \nnaive                 0.80\nfew_shot              0.70\nchain_of_thought      0.75\n","output_type":"stream"}]},{"cell_type":"code","source":"def save_gens_to_csv(gen, filename):\n    df = pd.DataFrame(gen, columns=[\"prompt\", \"output\", \"true_ans\", \"pred_ans\"])\n    df.to_csv(filename)\n    return df","metadata":{"id":"FQ5rPQeMIJaU","execution":{"iopub.status.busy":"2024-03-02T18:03:58.283456Z","iopub.execute_input":"2024-03-02T18:03:58.283724Z","iopub.status.idle":"2024-03-02T18:03:58.288413Z","shell.execute_reply.started":"2024-03-02T18:03:58.283702Z","shell.execute_reply":"2024-03-02T18:03:58.287498Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"n_out = save_gens_to_csv(orig_naive_gen, \"naive_outputs.csv\")\nfw_out = save_gens_to_csv(orig_fw_gen, \"few_shot_outputs.csv\")\ncot_out = save_gens_to_csv(orig_cot_gen, \"chain_of_thoughts_outputs.csv\")","metadata":{"id":"0Ilq5FopOlcF","execution":{"iopub.status.busy":"2024-03-02T18:03:58.289445Z","iopub.execute_input":"2024-03-02T18:03:58.289746Z","iopub.status.idle":"2024-03-02T18:03:58.318364Z","shell.execute_reply.started":"2024-03-02T18:03:58.289714Z","shell.execute_reply":"2024-03-02T18:03:58.317631Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"TODO: Try and compare \"naive\" prompting (your best hand-crafted variant), few-shot prompting (https://www.promptingguide.ai/techniques/fewshot) and chain-of-thought prompting (step-be-step thinking - https://www.promptingguide.ai/techniques/cot).\n\nTODO: Save the generation results into separate csv files and do not forget to attach them to your homework.","metadata":{"id":"WdLcYVkLIJaU"}},{"cell_type":"markdown","source":"# Part 2 (5 points): Fine-tuning with PEFT and LoRA","metadata":{"id":"rEyaUp9FIJaU"}},{"cell_type":"code","source":"from peft import LoraConfig\n\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"lm_head\", \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n)\npeft_model = peft.get_peft_model(model, peft_config)","metadata":{"id":"C6ciLPzoqqws","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_model.print_trainable_parameters()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2fi8qcS6qw5t","outputId":"95a7b173-1bab-4bb3-cf6d-dc56edba9916","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, pad_token_id=3)\ntokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"id":"t1LmHc3SqzPS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arguments = TrainingArguments(\n    output_dir=\"./tuned\",\n    report_to=\"tensorboard\",\n    logging_steps=5,\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    learning_rate=2e-5,\n    seed=224,\n    evaluation_strategy=\"no\",\n)","metadata":{"id":"ejCLO0Szq2PY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"","metadata":{"id":"J_6kuu3yLxdk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install --quiet trl","metadata":{"id":"BlWA9tvYwsOb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer, DataCollatorForCompletionOnlyLM","metadata":{"id":"aa_9L0bKq4BA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_idx = random.sample(range(1, 3270), 2000)\ntuned_ds = df[\"train\"].select(tuned_idx)","metadata":{"id":"QUycZ1-6q5kJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def formatting_func(example):\n    output_texts = []\n\n    for i in range(len(example[\"question\"])):\n        text = f\"\"\"### Question: {example[\"question\"][i]}?\\n ###Text: {example[\"passage\"][i]}\\n\n        ### Answer: {example[\"answer\"][i]}\\n\"\"\"\n        output_texts.append(text)\n    return output_texts\n\nresponse_template = \"### Answer:\"\ncollator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)","metadata":{"id":"x_NpYvOBq7Ho","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=peft_model,\n    train_dataset=tuned_ds,\n    peft_config=peft_config,\n    args=arguments,\n    formatting_func=formatting_func,\n    data_collator=collator,\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7-XEjvv5q8jG","outputId":"21b0171a-aef4-480e-b7ca-2690a0fcacf4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":94},"id":"oen46aCdq98S","outputId":"35af8d73-9003-428b-b4c0-258ad64bbea1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model(\"./tuned_model\")","metadata":{"id":"ESFhQ4Xmq_Xo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd ./tuned_model","metadata":{"id":"9qDkRRY7rI1v","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r tuned_model.zip .","metadata":{"id":"Nb6jyr7C5Pkn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig","metadata":{"id":"FULE8p7jreYn","execution":{"iopub.status.busy":"2024-03-02T18:05:20.434280Z","iopub.execute_input":"2024-03-02T18:05:20.435065Z","iopub.status.idle":"2024-03-02T18:05:20.439596Z","shell.execute_reply.started":"2024-03-02T18:05:20.435027Z","shell.execute_reply":"2024-03-02T18:05:20.438611Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"%ls","metadata":{"id":"4SOhKVM75lfV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_path = \"/kaggle/input/tuned-model/pytorch/tunedmodel/1\"\n\nconfig = PeftConfig.from_pretrained(peft_path)\n\ntmodel = PeftModel.from_pretrained(model, peft_path)","metadata":{"id":"yqAwqRDMrXvm","execution":{"iopub.status.busy":"2024-03-02T18:06:16.605455Z","iopub.execute_input":"2024-03-02T18:06:16.606142Z","iopub.status.idle":"2024-03-02T18:06:31.813220Z","shell.execute_reply.started":"2024-03-02T18:06:16.606107Z","shell.execute_reply":"2024-03-02T18:06:31.812375Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"tmodel.gradient_checkpointing_enable()\ntmodel.enable_input_require_grads()\ntmodel.to(device)","metadata":{"id":"86aUH6EYrqME","execution":{"iopub.status.busy":"2024-03-02T18:07:02.909294Z","iopub.execute_input":"2024-03-02T18:07:02.910558Z","iopub.status.idle":"2024-03-02T18:07:02.952371Z","shell.execute_reply.started":"2024-03-02T18:07:02.910524Z","shell.execute_reply":"2024-03-02T18:07:02.951387Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralSdpaAttention(\n              (q_proj): Linear4bit(\n                in_features=4096, out_features=4096, bias=False\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): Linear4bit(\n                in_features=4096, out_features=1024, bias=False\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): Linear4bit(\n                in_features=4096, out_features=1024, bias=False\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): Linear4bit(\n                in_features=4096, out_features=4096, bias=False\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm()\n            (post_attention_layernorm): MistralRMSNorm()\n          )\n        )\n        (norm): MistralRMSNorm()\n      )\n      (lm_head): Linear(\n        in_features=4096, out_features=32000, bias=False\n        (lora_dropout): ModuleDict(\n          (default): Dropout(p=0.05, inplace=False)\n        )\n        (lora_A): ModuleDict(\n          (default): Linear(in_features=4096, out_features=16, bias=False)\n        )\n        (lora_B): ModuleDict(\n          (default): Linear(in_features=16, out_features=32000, bias=False)\n        )\n        (lora_embedding_A): ParameterDict()\n        (lora_embedding_B): ParameterDict()\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def get_answer_for_peft(tokenizer, model, messages, max_new_tokens=200,\n               temperature=0.5, do_sample=True):\n    # TODO: tokenize input, generate answer and decode output. Pay attention to tokenizer methods\n    tokenized_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    model_output = model.generate(input_ids=tokenized_input, max_new_tokens=max_new_tokens, do_sample=do_sample, top_k=10, top_p=0.91)\n    decoded_output = tokenizer.batch_decode(model_output, skip_special_tokens=True)\n    return decoded_output","metadata":{"id":"FGQ4b_-GryWf","execution":{"iopub.status.busy":"2024-03-02T18:07:10.317697Z","iopub.execute_input":"2024-03-02T18:07:10.318065Z","iopub.status.idle":"2024-03-02T18:07:10.324302Z","shell.execute_reply.started":"2024-03-02T18:07:10.318037Z","shell.execute_reply":"2024-03-02T18:07:10.323310Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"tmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:07:13.036784Z","iopub.execute_input":"2024-03-02T18:07:13.037692Z","iopub.status.idle":"2024-03-02T18:07:13.071297Z","shell.execute_reply.started":"2024-03-02T18:07:13.037656Z","shell.execute_reply":"2024-03-02T18:07:13.070326Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralSdpaAttention(\n              (q_proj): Linear4bit(\n                in_features=4096, out_features=4096, bias=False\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): Linear4bit(\n                in_features=4096, out_features=1024, bias=False\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): Linear4bit(\n                in_features=4096, out_features=1024, bias=False\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): Linear4bit(\n                in_features=4096, out_features=4096, bias=False\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm()\n            (post_attention_layernorm): MistralRMSNorm()\n          )\n        )\n        (norm): MistralRMSNorm()\n      )\n      (lm_head): Linear(\n        in_features=4096, out_features=32000, bias=False\n        (lora_dropout): ModuleDict(\n          (default): Dropout(p=0.05, inplace=False)\n        )\n        (lora_A): ModuleDict(\n          (default): Linear(in_features=4096, out_features=16, bias=False)\n        )\n        (lora_B): ModuleDict(\n          (default): Linear(in_features=16, out_features=32000, bias=False)\n        )\n        (lora_embedding_A): ParameterDict()\n        (lora_embedding_B): ParameterDict()\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"tuned_naive_pred, tuned_naive_true, tuned_naive_gen = make_naive_predict(df_sample, tmodel, get_answer_fn=get_answer_for_peft)","metadata":{"id":"CMQooTb8r05Z","execution":{"iopub.status.busy":"2024-03-02T18:07:15.821517Z","iopub.execute_input":"2024-03-02T18:07:15.821889Z","iopub.status.idle":"2024-03-02T18:12:08.808019Z","shell.execute_reply.started":"2024-03-02T18:07:15.821861Z","shell.execute_reply":"2024-03-02T18:12:08.807187Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"tuned_fw_pred, tuned_fw_true, tuned_fw_gen = few_shot_prompting(df_sample, tmodel, get_answer_fn=get_answer_for_peft)","metadata":{"id":"eGY26UjIr2wd","execution":{"iopub.status.busy":"2024-03-02T18:12:08.809740Z","iopub.execute_input":"2024-03-02T18:12:08.810034Z","iopub.status.idle":"2024-03-02T18:16:13.233611Z","shell.execute_reply.started":"2024-03-02T18:12:08.810008Z","shell.execute_reply":"2024-03-02T18:16:13.232683Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"tuned_cot_pred, tuned_cot_true, tuned_cot_gen = cot_prompting(df_sample, tmodel, get_answer_fn=get_answer_for_peft)","metadata":{"id":"9JvCyzQwsI6Y","execution":{"iopub.status.busy":"2024-03-02T18:16:13.234869Z","iopub.execute_input":"2024-03-02T18:16:13.235186Z","iopub.status.idle":"2024-03-02T18:18:15.155598Z","shell.execute_reply.started":"2024-03-02T18:16:13.235146Z","shell.execute_reply":"2024-03-02T18:18:15.154740Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"orig_metrics","metadata":{"id":"yt8jikMwsNmI","execution":{"iopub.status.busy":"2024-03-02T18:18:15.157834Z","iopub.execute_input":"2024-03-02T18:18:15.158526Z","iopub.status.idle":"2024-03-02T18:18:15.170359Z","shell.execute_reply.started":"2024-03-02T18:18:15.158476Z","shell.execute_reply":"2024-03-02T18:18:15.169329Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"                  accuracy\ntitle                     \nnaive                 0.80\nfew_shot              0.70\nchain_of_thought      0.75","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>accuracy</th>\n    </tr>\n    <tr>\n      <th>title</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>naive</th>\n      <td>0.80</td>\n    </tr>\n    <tr>\n      <th>few_shot</th>\n      <td>0.70</td>\n    </tr>\n    <tr>\n      <th>chain_of_thought</th>\n      <td>0.75</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"tuned_metrics = evaluate_answers([tuned_naive_true, tuned_fw_true, tuned_cot_true], [tuned_naive_pred, tuned_fw_pred, tuned_cot_pred])","metadata":{"id":"UiyaaItWsQoH","execution":{"iopub.status.busy":"2024-03-02T18:18:15.171571Z","iopub.execute_input":"2024-03-02T18:18:15.171876Z","iopub.status.idle":"2024-03-02T18:18:15.184011Z","shell.execute_reply.started":"2024-03-02T18:18:15.171850Z","shell.execute_reply":"2024-03-02T18:18:15.182992Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"                  accuracy\ntitle                     \nnaive                 0.75\nfew_shot              0.80\nchain_of_thought      0.80\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd ../","metadata":{"id":"UfDurM4I8xlN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_out_tune = save_gens_to_csv(tuned_naive_gen, \"tuned_naive_outputs.csv\")\nfw_out_tune = save_gens_to_csv(tuned_fw_gen, \"tuned_few_shot_outputs.csv\")\ncot_out_tune = save_gens_to_csv(tuned_cot_gen, \"tuned_chain_of_thoughts_outputs.csv\")","metadata":{"id":"Cp1e3gxXsaUD","execution":{"iopub.status.busy":"2024-03-02T18:19:36.441289Z","iopub.execute_input":"2024-03-02T18:19:36.441740Z","iopub.status.idle":"2024-03-02T18:19:36.466265Z","shell.execute_reply.started":"2024-03-02T18:19:36.441689Z","shell.execute_reply":"2024-03-02T18:19:36.465200Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"TODO: initialize Trainer and pass train part of our dataset for 2-3 epoches\n\nNote: carefully set max_seq_length and args (that are transformers.TrainingArguments)","metadata":{"id":"tONec3-xIJaV"}},{"cell_type":"markdown","source":"TODO: save and check your tuned model. Provide scores on our 20 validation examples and save result to csv file","metadata":{"id":"ouReWkZTIJaV"}}]}